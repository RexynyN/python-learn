{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep as sleep \n",
    "from os.path import join\n",
    "import pyautogui as pag\n",
    "import pyperclip as clip\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================== CONFIGS ==========================================\n",
    "\n",
    "# If there is a variable path, put a \"???\" to be replaced by the value in \"VARIABLE_PATH\"\n",
    "BASE_URL = \"google.com\"\n",
    "\n",
    "# Variable Path\n",
    "VARIABLE_PATH = [\n",
    "    \"\"\n",
    "]\n",
    "\n",
    "PATH = join(\"..\", \"<path>\")\n",
    "\n",
    "# Ignores a variable that already has a subdirectory in the PATH\n",
    "IGNORE_DUPLICATES = True\n",
    "\n",
    "# ========================================================================================\n",
    "\n",
    "len(VARIABLE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def script(path):\n",
    "    script = \"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        script = f.read()\n",
    "\n",
    "    return script\n",
    "\n",
    "def assert_path(path) -> bool:\n",
    "    is_path = os.path.isdir(path)\n",
    "    if not is_path:\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    return is_path\n",
    "\n",
    "def fetch(path, image_url):\n",
    "    try:\n",
    "        print(image_url)\n",
    "        r = requests.get(image_url, stream = True)\n",
    "        r.raw.decode_content = True\n",
    "\n",
    "        name = image_url.split(\"/\")[-1]\n",
    "\n",
    "        with open(join(path, name),'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "    except:\n",
    "        fetch(path, image_url)\n",
    "\n",
    "def fetch_all(path, data):\n",
    "    assert_path(path)\n",
    "\n",
    "    for image_url in data:\n",
    "        fetch(path, image_url)\n",
    "        \n",
    "\n",
    "# Fetch a single Image \n",
    "# https://stackoverflow.com/questions/35388332/how-to-download-images-with-aiohttp\n",
    "async def async_fetch(session: aiohttp.ClientSession, url: str, path: str) -> str:\n",
    "    async with session.get(url) as resp:\n",
    "        if resp.status == 200:\n",
    "            name = url.split(\"/\")[-1]\n",
    "            async with aiofiles.open(join(path, name), mode='wb') as f:\n",
    "                await f.write(await resp.read())\n",
    "            return \"\"\n",
    "        else:\n",
    "            return url\n",
    "\n",
    "# Fetch all URLs\n",
    "async def async_fetch_all(session: aiohttp.ClientSession, urls: str, path: str) -> list[str]:\n",
    "    assert_path(path)\n",
    "    tasks = []\n",
    "    for url in urls:\n",
    "        task = asyncio.create_task(async_fetch(session, url, path))\n",
    "        tasks.append(task)\n",
    "    \n",
    "    # Return the unsuccessfull requests\n",
    "    res = await asyncio.gather(*tasks)\n",
    "    res = [url for url in res if url != \"\"]\n",
    "    return res \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dir = os.listdir(PATH)\n",
    "length = len(VARIABLE_PATH)\n",
    "done = 1\n",
    "pag.PAUSE = 0.3\n",
    "\n",
    "len(os.listdir(\"../kill_me_when_seen/\"))\n",
    "JSON_PATH = join(PATH, \"jsons\")\n",
    "assert_path(JSON_PATH)\n",
    "\n",
    "# Gives 3 seconds to focus on a browser window (use chrome, to be sure)\n",
    "sleep(3)\n",
    "\n",
    "for urls in VARIABLE_PATH: # VARIABLE_PATH:\n",
    "    print(f\"{urls} => {done} / {length}\")\n",
    "\n",
    "    if urls in var_dir or f\"{urls}.json\" in os.listdir(JSON_PATH):\n",
    "        print(\"SKIPPED - Already has a subdirectory or json\")\n",
    "        done += 1\n",
    "        continue \n",
    "\n",
    "    # Focus on the search bar and puts the URL\n",
    "    pag.hotkey('ctrl', 'l', interval=0.3)\n",
    "    clip.copy(BASE_URL.replace(\"???\", str(urls)))\n",
    "    pag.hotkey(\"ctrl\", \"v\", interval=0.3)\n",
    "    pag.press(\"enter\")\n",
    "\n",
    "    # Awaits for the site to load\n",
    "    sleep(5)\n",
    "\n",
    "    # Opens terminal\n",
    "    pag.hotkey('ctrl', 'shift', 'j')\n",
    "\n",
    "    # Focus on console\n",
    "    pag.hotkey(\"ctrl\", \"'\", interval=0.5)\n",
    "\n",
    "    # Puts the script\n",
    "    clip.copy(script(\"../clover.js\"))\n",
    "    pag.hotkey(\"ctrl\", \"v\", interval=0.5)\n",
    "    pag.press(\"enter\")\n",
    "\n",
    "    # Focus on the window, for the clipboard script to work\n",
    "    pag.hotkey(\"alt\", \"tab\", interval=0.5)\n",
    "    sleep(6)\n",
    "    \n",
    "    # Pastes the returning output\n",
    "    data = clip.paste()\n",
    "    data = data.split(\",\")\n",
    "    \n",
    "    with open(join(JSON_PATH, f\"{urls}.json\"), \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    # Sleep, because the program needs a rest too...\n",
    "    sleep(3)\n",
    "    done += 1\n",
    "\n",
    "# Rick rolls, for some reason\n",
    "pag.hotkey('ctrl', 'l', interval=0.3)\n",
    "clip.copy(\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n",
    "pag.hotkey(\"ctrl\", \"v\", interval=0.3)\n",
    "pag.press(\"enter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = 1\n",
    "length = len(VARIABLE_PATH)\n",
    "for url in VARIABLE_PATH:\n",
    "    print(f\"{url} => {done} / {length}\")\n",
    "    # Read the json, if it doesn't exists, continue the loop\n",
    "    try:\n",
    "        with open(join(JSON_PATH, f\"{url}.json\"), \"r\") as f:\n",
    "            leftovers = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        done += 1\n",
    "        continue\n",
    "\n",
    "    if not leftovers:\n",
    "        done += 1\n",
    "        continue \n",
    "\n",
    "    # Looping until all the urls have been taken care of\n",
    "    while leftovers:\n",
    "        # Do whatever with the scraped data\n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                leftovers = await async_fetch_all(session, leftovers, join(PATH, url))\n",
    "                print(leftovers)\n",
    "                # # Handling unsuccessfull requests in an async environment is tough, so just let the sync version do that  \n",
    "                # if leftovers:\n",
    "                #     fetch_all(join(PATH, urls), leftovers)\n",
    "        except aiohttp.InvalidURL: \n",
    "            break\n",
    "        except TimeoutError:\n",
    "            print(\"Oopsie :()\")\n",
    "    # Remove the json\n",
    "    # os.remove(join(PATH, url, f\"{url}.json\"))\n",
    "    done += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "72b2382ece9768098284d92bbc69d35954e75b60d1e25897d1389c232f4796f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
